{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb9b918",
   "metadata": {},
   "source": [
    "There are four main types of prompts that we can use for prompting.\n",
    "1. Using a <b>RolePrompt</b> Template - Defines the expectation from the model in a \"role-playing\" mode and provides the placeholders for user input.\n",
    "2. Using a <b>Few shot Prompt</b> Template - give examples to the model in terms of what is expected.\n",
    "3. Using <b>Chain Prompting</b> Templates - Provide a set of sequential prompts where the response of prompt 1, for example, is the input for prompt 2\n",
    "4. Using <b>Chain of Reasoning</b> Templates - Encourage the model to provide step-by-step description of its \"thought\" process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292eb2fd",
   "metadata": {},
   "source": [
    "## RolePrompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ef65b",
   "metadata": {},
   "source": [
    "Provide a role to the model to help it finetune its response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73df783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.10) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f92cb1",
   "metadata": {},
   "source": [
    "#### Create the role and define what the user inputs will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639fc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role='You are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about {theme} in the genre {genre} for the year {year}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48a566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['theme','genre','year'] #These are the three input variables that the user will give at every interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa3128",
   "metadata": {},
   "source": [
    "#### Create the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c204de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=role, input_variables=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55fc37eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about starry skies in the genre electronica for the year 2030')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Example of how the prompt will look when it is submitted to the model\n",
    "prompt.format_prompt(theme='starry skies',genre='electronica',year='2030')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af5c12",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1317431",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='text-davinci-003',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a4ea9",
   "metadata": {},
   "source": [
    "#### Create a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed04751",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(llm=llm, prompt=prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbfd0",
   "metadata": {},
   "source": [
    "#### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1b79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about night sky in the genre electronica for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Starry Nights 2030\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='night sky',genre='electronica',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99005fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about starry sky in the genre electronica for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Starry Skies 2030: An Electronica Odyssey\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='starry sky',genre='electronica',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9222cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about starry sky in the genre rock for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Starry Skies of 2030\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='starry sky',genre='rock',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401191f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about interstellar travel in the genre rock for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Interstellar Odyssey: A Rock Anthem for 2030\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='interstellar travel',genre='rock',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218dcdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about interstellar travel in the genre rap for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Interstellar Hustle: A Rap Odyssey of the Future\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='interstellar travel',genre='rap',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41e50f",
   "metadata": {},
   "source": [
    "<b>The above titles while relevant sound incredibly boring. Maybe, if we try increasing the temperature of the LLM? The temperature metric allows the model to be more creative and diverge away from the prompt while anchoring itself to it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c28381",
   "metadata": {},
   "source": [
    "#### Changing the temperature parameter of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ef56f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='text-davinci-003',temperature=0.9)\n",
    "llmchain=LLMChain(llm=llm, prompt=prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5876c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about interstellar travel in the genre rock for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Moon Light Drive 2050\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='interstellar travel',genre='rock',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc96b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a futuristic song writer who specializes in generating creative song titles. I need your help in creating a song title about interstellar travel in the genre rap for the year 2030\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ".\n",
      "\n",
      "\"Journey Through the Starlight: A Rap about Interstellar Travel in 2030\"\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(theme='interstellar travel',genre='rap',year='2030')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb84c96",
   "metadata": {},
   "source": [
    "<b>The names actually sound far more album-ish and creative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12fd26",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b500cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142519a",
   "metadata": {},
   "source": [
    "The FewShotPrompt structure looks a bit like this:<br>\n",
    "<ul>\n",
    "    <li>< prompt > The structure of the input data. This structure should be similar to the examples provided</li>\n",
    "    <li>< prefix for the examples> Tell the model that we want outputs that are similar to the examples given</li>\n",
    "    <li>< examples > The actual examples</li>\n",
    "    <li>< suffix for the examples > Give the format that we want the output in</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af36e2",
   "metadata": {},
   "source": [
    "#### Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9ac1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "inputs=['color','emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0934cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(input_variables=inputs, template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f04e4f",
   "metadata": {},
   "source": [
    "#### Create the prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12be5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='Here are some examples of colors and the emotions associated with them\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d318a08",
   "metadata": {},
   "source": [
    "#### Create the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a232ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each example needs to be a dictionary in the same format as the template\n",
    "examples=[\n",
    "    {\"color\":\"red\", \"emotion\":\"anger\"},\n",
    "    {\"color\":\"blue\" , \"emotion\":\"serenity\"},\n",
    "    {\"color\":\"green\",\"emotion\":\"tranquility\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5324608",
   "metadata": {},
   "source": [
    "#### Create the suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b6f264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix= \"Given a new color, use the examples provided to identify the emotion for the color {color} in the format:\\n\\nColor: {color}\\nEmotion:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4192b80",
   "metadata": {},
   "source": [
    "#### Create the FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adcf2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt=FewShotPromptTemplate(example_prompt=prompt,prefix=prefix,suffix=suffix,examples=examples,input_variables=['color'],example_separator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c6b1e",
   "metadata": {},
   "source": [
    "#### What does the final prompt look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1cb6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Here are some examples of colors and the emotions associated with them\\n\\n\\n\\nColor: red\\nEmotion: anger\\n\\n\\n\\nColor: blue\\nEmotion: serenity\\n\\n\\n\\nColor: green\\nEmotion: tranquility\\n\\n\\nGiven a new color, use the examples provided to identify the emotion for the color purple in the format:\\n\\nColor: purple\\nEmotion:'\n"
     ]
    }
   ],
   "source": [
    "print(fspt.format_prompt(color='purple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6954fb0",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8b925e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874985d",
   "metadata": {},
   "source": [
    "#### Create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "807dde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(llm=llm, prompt=fspt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77e8d5",
   "metadata": {},
   "source": [
    "#### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5243ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " calmness\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(color='purple')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea28364",
   "metadata": {},
   "source": [
    "#### What if we change the temperature to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f8cb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)\n",
    "llmchain=LLMChain(llm=llm, prompt=fspt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f4bcdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " calmness\n"
     ]
    }
   ],
   "source": [
    "response=llmchain.run(color='purple')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e344d",
   "metadata": {},
   "source": [
    "<b> The response does not seem to change much based on the temperature. However, calmness seems to be an average of anger and serenity, the same way that purple can be construed as the average of red and blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03fce7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\tEmotion:  optimism\n",
      "Color: orange\tEmotion:  optimism\n",
      "Color: Brown\tEmotion:  warmth\n"
     ]
    }
   ],
   "source": [
    "print('Color: yellow\\tEmotion:',llmchain.run(color='yellow'))\n",
    "print('Color: orange\\tEmotion:',llmchain.run(color='orange'))\n",
    "print('Color: Brown\\tEmotion:',llmchain.run(color='Brown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "523f86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\tEmotion:  optimism\n",
      "Color: orange\tEmotion:  optimism\n",
      "Color: Brown\tEmotion:  warmth\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0.9)\n",
    "llmchain=LLMChain(llm=llm, prompt=fspt)\n",
    "print('Color: yellow\\tEmotion:',llmchain.run(color='yellow'))\n",
    "print('Color: orange\\tEmotion:',llmchain.run(color='orange'))\n",
    "print('Color: Brown\\tEmotion:',llmchain.run(color='Brown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767fcab",
   "metadata": {},
   "source": [
    "So yellow and orange seem to be strongly correlated. <br>\n",
    "<b>However, the exercise does highlight the slight instability of the model's word generation prowess as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5047856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\tEmotion:  optimism\n",
      "Color: orange\tEmotion:  optimism\n",
      "Color: Brown\tEmotion:  stability\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0.9)\n",
    "llmchain=LLMChain(llm=llm, prompt=fspt)\n",
    "print('Color: yellow\\tEmotion:',llmchain.run(color='yellow'))\n",
    "print('Color: orange\\tEmotion:',llmchain.run(color='orange'))\n",
    "print('Color: Brown\\tEmotion:',llmchain.run(color='Brown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2936c4b8",
   "metadata": {},
   "source": [
    "All we did was re-perform the exercise with no change between the previous two cells. However the emotion of <b>brown</b> changed from <b>warmth</b> to <b>stability</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6f0b4",
   "metadata": {},
   "source": [
    "#### Change temperature to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57a06c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\tEmotion:  optimism\n",
      "Color: orange\tEmotion:  optimism\n",
      "Color: Brown\tEmotion:  warmth\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)\n",
    "llmchain=LLMChain(llm=llm, prompt=fspt)\n",
    "print('Color: yellow\\tEmotion:',llmchain.run(color='yellow'))\n",
    "print('Color: orange\\tEmotion:',llmchain.run(color='orange'))\n",
    "print('Color: Brown\\tEmotion:',llmchain.run(color='Brown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "273dbd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: yellow\tEmotion:  optimism\n",
      "Color: orange\tEmotion:  optimism\n",
      "Color: Brown\tEmotion:  warmth\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)\n",
    "llmchain=LLMChain(llm=llm, prompt=fspt)\n",
    "print('Color: yellow\\tEmotion:',llmchain.run(color='yellow'))\n",
    "print('Color: orange\\tEmotion:',llmchain.run(color='orange'))\n",
    "print('Color: Brown\\tEmotion:',llmchain.run(color='Brown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d5de7",
   "metadata": {},
   "source": [
    "So keeping the temperature = 0 not only ensures adherence to the prompt but also increases the stability of the response over multiple interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee261c55",
   "metadata": {},
   "source": [
    "## ChainPrompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ae03d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a1b37",
   "metadata": {},
   "source": [
    "#### Create two prompts to be chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15673c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1='What is the name of the scientist who was responsible for the raman effect?'\n",
    "prompt1=PromptTemplate(\n",
    "    template=template1,\n",
    "    input_variables=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "757a8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2='What were the other discoveries attributed to {scientist}?'\n",
    "prompt2=PromptTemplate(\n",
    "template=template2,\n",
    "input_variables=['scientist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880c57d",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b5b387b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a1f20",
   "metadata": {},
   "source": [
    "#### Create the chain and run it for the first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91a7706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain1=LLMChain(prompt=prompt1,llm=llm)\n",
    "response=llmchain1.run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7acd9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scientist responsible for the Raman effect is Indian physicist Sir Chandrasekhara Venkata Raman.\n"
     ]
    }
   ],
   "source": [
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfef17a",
   "metadata": {},
   "source": [
    "Getting the entire sentence as the answer instead of just the name won't act well as the input data for our second prompt.<br> <b> A hack to get only the name is to structure the prompt as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c7704d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1a=\"\"\"What is the name of the scientist who was responsible for the raman effect? Answer: \"\"\"\n",
    "prompt1a=PromptTemplate(\n",
    "    template=template1a,\n",
    "    input_variables=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ab8a3840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Chandrasekhara Venkata Raman (C.V. Raman)\n"
     ]
    }
   ],
   "source": [
    "llmchain1=LLMChain(prompt=prompt1a,llm=llm)\n",
    "response1=llmchain1.run({})\n",
    "print(response1.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110c9e3",
   "metadata": {},
   "source": [
    "#### Chaining the second prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdd44446",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain2=LLMChain(prompt=prompt2, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f57f49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=llmchain2(response1.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6fbcfe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scientist': 'Sir Chandrasekhara Venkata Raman (C.V. Raman)', 'text': '\\n\\nIn addition to his Nobel Prize-winning discovery of the Raman Effect, C.V. Raman made several other important contributions to the field of physics. These include the discovery of the Raman-Nath diffraction, the Raman-Nath theory of diffraction, the Raman-Kashyap effect, the Raman-Kashyap theory of diffraction, the Raman-Kashyap-Raman effect, the Raman-Kashyap-Raman theory of diffraction, the Raman-Kashyap-Raman-Nath effect, the Raman-Kashyap-Raman-Nath theory of diffraction, the Raman-Kashyap-Raman-Nath-Raman effect, the Raman-Kashyap-Raman-Nath-Raman theory of diffraction, the Raman-Kashyap-Raman-Nath-Raman-Kashyap effect, and the Raman-Kashyap-Raman-Nath-Raman-Kashyap theory of diffraction. He also made important contributions to the fields of optics, acoustics, and spect'}\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "48be0b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Chandrasekhara Venkata Raman (C.V. Raman) \n",
      " \n",
      "\n",
      "In addition to his Nobel Prize-winning discovery of the Raman Effect, C.V. Raman made several other important contributions to the field of physics. These include the discovery of the Raman-Nath diffraction, the Raman-Nath theory of diffraction, the Raman-Kashyap effect, the Raman-Kashyap theory of diffraction, the Raman-Kashyap-Raman effect, the Raman-Kashyap-Raman theory of diffraction, the Raman-Kashyap-Raman-Nath effect, the Raman-Kashyap-Raman-Nath theory of diffraction, the Raman-Kashyap-Raman-Nath-Raman effect, the Raman-Kashyap-Raman-Nath-Raman theory of diffraction, the Raman-Kashyap-Raman-Nath-Raman-Kashyap effect, and the Raman-Kashyap-Raman-Nath-Raman-Kashyap theory of diffraction. He also made important contributions to the fields of optics, acoustics, and spect\n"
     ]
    }
   ],
   "source": [
    "print(response2['scientist'],'\\n',response2['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c9b5b",
   "metadata": {},
   "source": [
    "#### Trying to chain the second prompt with 'response' (The full sentence answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4fafc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scientist': 'The scientist responsible for the Raman effect is Indian physicist Sir Chandrasekhara Venkata Raman.', 'text': '\\n\\nOther discoveries attributed to Sir Chandrasekhara Venkata Raman include:\\n\\n1. The Raman-Nath theory of diffraction of light\\n2. The Raman-Nath effect\\n3. The Raman-Sommerfeld theory of diffraction\\n4. The Raman-Nath effect in X-rays\\n5. The Raman-Nath effect in electron diffraction\\n6. The Raman-Nath effect in neutron diffraction\\n7. The Raman-Nath effect in acoustic waves\\n8. The Raman-Nath effect in surface waves\\n9. The Raman-Nath effect in optical fibers\\n10. The Raman-Nath effect in semiconductors\\n11. The Raman-Nath effect in superconductors\\n12. The Raman-Nath effect in plasmas\\n13. The Raman-Nath effect in liquid crystals\\n14. The Raman-Nath effect in polymers\\n15. The Raman-Nath effect in nanostructures\\n16. The Raman-Nath effect in nanomaterials\\n17. The Raman-Nath effect in nanoelectronics\\n18.'}\n"
     ]
    }
   ],
   "source": [
    "response3=llmchain2(response.strip())\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd77fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scientist responsible for the Raman effect is Indian physicist Sir Chandrasekhara Venkata Raman. \n",
      " \n",
      "\n",
      "Other discoveries attributed to Sir Chandrasekhara Venkata Raman include:\n",
      "\n",
      "1. The Raman-Nath theory of diffraction of light\n",
      "2. The Raman-Nath effect\n",
      "3. The Raman-Sommerfeld theory of diffraction\n",
      "4. The Raman-Nath effect in X-rays\n",
      "5. The Raman-Nath effect in electron diffraction\n",
      "6. The Raman-Nath effect in neutron diffraction\n",
      "7. The Raman-Nath effect in acoustic waves\n",
      "8. The Raman-Nath effect in surface waves\n",
      "9. The Raman-Nath effect in optical fibers\n",
      "10. The Raman-Nath effect in semiconductors\n",
      "11. The Raman-Nath effect in superconductors\n",
      "12. The Raman-Nath effect in plasmas\n",
      "13. The Raman-Nath effect in liquid crystals\n",
      "14. The Raman-Nath effect in polymers\n",
      "15. The Raman-Nath effect in nanostructures\n",
      "16. The Raman-Nath effect in nanomaterials\n",
      "17. The Raman-Nath effect in nanoelectronics\n",
      "18.\n"
     ]
    }
   ],
   "source": [
    "print(response3['scientist'],'\\n',response3['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952de185",
   "metadata": {},
   "source": [
    "<b>Wow! Two different input structures but it still works! Ironically, the second prompt is better formatted than the first !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a56571",
   "metadata": {},
   "source": [
    "## Chain of Thought  Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c6232",
   "metadata": {},
   "source": [
    "Chain of Thought Prompting (CoT) is a technique developed to encourage large language models to explain their reasoning process, <a href=\"https://arxiv.org/abs/2201.11903\">leading to more accurate results</a>.<br> Chain of Thought template can be designed by simply putting together for e.g. a chain of few shot prompt templates <br>\n",
    "However, while sharing the examples, e.g. {q: < enter the question >, a: < enter the answer >}, the value of the key 'a' <b>should not just provide the answer but also the steps taken to arrive at the answer</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d8ae189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915f56b",
   "metadata": {},
   "source": [
    "### Using only prompts without FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459cbce",
   "metadata": {},
   "source": [
    "#### Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f0b51038",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\\nAnswer:'\n",
    "prompt=PromptTemplate(template=template, input_variables=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4316b",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7532f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664c18b",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e62bbbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\\nAnswer:', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cd445069",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm(prompt=prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8aa5141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12 apples\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83bbe0",
   "metadata": {},
   "source": [
    "<b><font color= 'red'>&#10006;</font></b><b> This is actually wrong! 10-2-2+5-1 = 10 apples !</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb4f8e",
   "metadata": {},
   "source": [
    "#### Now trying again with the step by step prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0c456149",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1='Question: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\\nAnswer: Let us think step by step'\n",
    "prompt1=PromptTemplate(template=template1, input_variables=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e8bce8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1=llm(prompt=prompt1.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "45449f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Initially, you had 10 apples.\n",
      "You gave 2 apples to the neighbor and 2 to the repairman.\n",
      "So, you had 10 - 2 - 2 = 6 apples left.\n",
      "You then bought 5 more apples.\n",
      "So, you had 6 + 5 = 11 apples.\n",
      "Finally, you ate 1 apple.\n",
      "So, you had 11 - 1 = 10 apples left.\n",
      "\n",
      "Therefore, you remained with 10 apples.\n"
     ]
    }
   ],
   "source": [
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd60025",
   "metadata": {},
   "source": [
    "<b>Welcome to the right answer. The only difference was the way we prompted the model to think step by step !</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744523e8",
   "metadata": {},
   "source": [
    "### Chain of thought reasoning using FewShotPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "901dade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "User: {question}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "55e9fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set_1=[\n",
    "    {'question':'I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?',\n",
    "     'answer':'10 apples'},\n",
    "    {'question':'While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?',\n",
    "    'answer':'24 songs'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8dbe879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1=PromptTemplate(template=example_template,input_variables=['question','answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8ad32aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"The following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1a29e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix='''user:{question}\n",
    "AI:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "75005c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt_math=FewShotPromptTemplate(prefix=prefix,examples=example_set_1, example_prompt=prompt_template1,suffix=suffix, input_variables=['question'],example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d3b730b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] output_parser=None partial_variables={} examples=[{'question': 'I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?', 'answer': '10 apples'}, {'question': 'While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?', 'answer': '24 songs'}] example_selector=None example_prompt=PromptTemplate(input_variables=['question', 'answer'], output_parser=None, partial_variables={}, template='\\nUser: {question}\\nAI: {answer}\\n', template_format='f-string', validate_template=True) suffix='user:{question}\\nAI:' example_separator='\\n\\n' prefix=\"The following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \" template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(fspt_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cfe24423",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_math=LLMChain(llm=llm, prompt=fspt_math,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a197aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1='Roger has 5 tennis balls. He buys two more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5d33faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \n",
      "\n",
      "\n",
      "User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "AI: 10 apples\n",
      "\n",
      "\n",
      "\n",
      "User: While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?\n",
      "AI: 24 songs\n",
      "\n",
      "\n",
      "user:Roger has 5 tennis balls. He buys two more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response_math=llmchain_math.run(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fd273d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11 tennis balls\n"
     ]
    }
   ],
   "source": [
    "print(response_math)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52ac21",
   "metadata": {},
   "source": [
    "<b><font color= 'green'>&#10004;</font></b> This is the right answer. We did not chain of reasoning for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "386ff6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2='A juggler can juggle 16 balls. Half the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fc4b7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \n",
      "\n",
      "\n",
      "User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "AI: 10 apples\n",
      "\n",
      "\n",
      "\n",
      "User: While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?\n",
      "AI: 24 songs\n",
      "\n",
      "\n",
      "user:A juggler can juggle 16 balls. Half the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response_math2=llmchain_math.run(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "16657519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 blue golf balls\n"
     ]
    }
   ],
   "source": [
    "print(response_math2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da6a72",
   "metadata": {},
   "source": [
    "<b><font color= 'red'>&#10006;</font></b> This the wrong answer. The correct answer is 0.5 * 0.5 * 16 = 4 blue balls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12163e54",
   "metadata": {},
   "source": [
    "<b><font color= 'green'>&#10004;</font></b><br> <b><font color= 'red'>&#10006;</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14bd62",
   "metadata": {},
   "source": [
    "#### Solving question2 with chain of thought reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9b36ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_template='''Please provide your step-by-step reasoning. User: {question}\n",
    "AI: Let's think step by step {answer}'''\n",
    "reason_prompt=PromptTemplate(input_variables=['question','answer'],template=reason_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"The following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f14955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set_2=[\n",
    "    {'question':'I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?',\n",
    "     'answer':'''Let's think step by step.First, you started with 10 apples. You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.\n",
    "    Then you bought 5 more apples, so now you had 11 apples. Finally, you ate 1 apple, so you would remain with 10 apples.'''},\n",
    "    {'question':'While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?',\n",
    "    'answer':\"Let's think step by step. Zoe bought 3 country albums. Each album has 3 songs. So she bought 3*3 = 9 songs from the country albums. Zoe bought 5 pop albums. Each album has 3 songs. So she bought 5*3 = 15 songs from the pop album. Zoe bought 9+15=24 songs in total. The answer is 24\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a425a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt_reason=FewShotPromptTemplate(\n",
    "    prefix=prefix, \n",
    "    examples=example_set_2, \n",
    "    example_prompt=reason_prompt, \n",
    "    suffix=suffix,\n",
    "    example_separator='\\n\\n',\n",
    "    input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "591a7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_reason=LLMChain(prompt=fspt_reason, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "526933bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \n",
      "\n",
      "Please provide your step-by-step reasoning. User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "AI: Let us think step by step Let's think step by step.First, you started with 10 apples. You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.\n",
      "    Then you bought 5 more apples, so now you had 11 apples. Finally, you ate 1 apple, so you would remain with 10 apples.\n",
      "\n",
      "Please provide your step-by-step reasoning. User: While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?\n",
      "AI: Let us think step by step Let's think step by step. Zoe bought 3 country albums. Each album has 3 songs. So she bought 3*3 = 9 songs from the country albums. Zoe bought 5 pop albums. Each album has 3 songs. So she bought 5*3 = 15 songs from the pop album. Zoe bought 9+15=24 songs in total. The answer is 24\n",
      "\n",
      "user:A juggler can juggle 16 balls. Half the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response_reason=llmchain_reason.run(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1988de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let us think step by step. The juggler can juggle 16 balls. Half of the balls are golf balls, so there are 8 golf balls. Half of the 8 golf balls are blue, so there are 4 blue golf balls. The answer is 4 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "print(response_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150860d",
   "metadata": {},
   "source": [
    "<b><font color= 'green'>&#10004;</font> This is indeed the correct answer !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d021cfe",
   "metadata": {},
   "source": [
    "#### Trying one last math question to contrast chain of reasoning and non-chain of reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "77ef3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "question3='A pet store has 64 puppies. In one day, they sold 28 of them and put the rest into cages with 4 in each cage. How many cages did they use?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad8f9f",
   "metadata": {},
   "source": [
    "#### Without chain of reason prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9e33585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \n",
      "\n",
      "\n",
      "User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "AI: 10 apples\n",
      "\n",
      "\n",
      "\n",
      "User: While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?\n",
      "AI: 24 songs\n",
      "\n",
      "\n",
      "user:A pet store has 64 puppies. In one day, they sold 28 of them and put the rest into cages with 4 in each cage. How many cages did they use?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 14 cages\n"
     ]
    }
   ],
   "source": [
    "response_math3=llmchain_math.run(question3)\n",
    "print(response_math3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c30183",
   "metadata": {},
   "source": [
    "<b><font color= 'red'>&#10006;</font></b> This the wrong answer. The correct answer is (64-28)/4=9 cages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f96e5d",
   "metadata": {},
   "source": [
    "#### With chain of reason prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7f99602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI math coach. The assistant provides answers to the users' questions. Here are some examples: \n",
      "\n",
      "Please provide your step-by-step reasoning. User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "AI: Let us think step by step Let's think step by step.First, you started with 10 apples. You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.\n",
      "    Then you bought 5 more apples, so now you had 11 apples. Finally, you ate 1 apple, so you would remain with 10 apples.\n",
      "\n",
      "Please provide your step-by-step reasoning. User: While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyrics sheet and had 3 songs. How many songs did Zoe buy in total?\n",
      "AI: Let us think step by step Let's think step by step. Zoe bought 3 country albums. Each album has 3 songs. So she bought 3*3 = 9 songs from the country albums. Zoe bought 5 pop albums. Each album has 3 songs. So she bought 5*3 = 15 songs from the pop album. Zoe bought 9+15=24 songs in total. The answer is 24\n",
      "\n",
      "user:A pet store has 64 puppies. In one day, they sold 28 of them and put the rest into cages with 4 in each cage. How many cages did they use?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Let us think step by step. The pet store had 64 puppies. They sold 28 of them, so they had 36 puppies left. Since each cage has 4 puppies, they would need to divide the 36 puppies into groups of 4. 36 divided by 4 is 9, so they used 9 cages. The answer is 9 cages.\n"
     ]
    }
   ],
   "source": [
    "response_reason3=llmchain_reason.run(question3)\n",
    "print(response_reason3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47211225",
   "metadata": {},
   "source": [
    "<b><font color= 'green'>&#10004;</font> This is indeed the correct answer !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
