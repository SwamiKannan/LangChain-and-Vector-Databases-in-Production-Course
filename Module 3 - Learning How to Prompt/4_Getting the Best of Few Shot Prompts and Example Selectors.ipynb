{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187945cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.11) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate,HumanMessagePromptTemplate,AIMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2d008",
   "metadata": {},
   "source": [
    "## Alternating human/system messages (Usually for Chat setups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fc0db",
   "metadata": {},
   "source": [
    "### I. Check if it can speak in pirate lingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c91cc3",
   "metadata": {},
   "source": [
    "#### Set up the System Message Prompt that defines the overall functionality of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670e81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_pirate='You are a helpful AI assistant who can translate English to pirate language'\n",
    "prompt_pirate=PromptTemplate(template=template_pirate,input_variables=[])\n",
    "smpt_pirate=SystemMessagePromptTemplate(prompt=prompt_pirate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e6051",
   "metadata": {},
   "source": [
    "#### Provide a \"chat\" interaction as a prompt for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138bfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmpt_pirate1=HumanMessagePromptTemplate.from_template('Hi')\n",
    "ai_mpt_pirate1=AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "template_pirate2='{text}'\n",
    "hmpt_pirate2=HumanMessagePromptTemplate.from_template('{text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76680d6f",
   "metadata": {},
   "source": [
    "#### Create the final chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ad39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_pirate=ChatPromptTemplate(messages=[smpt_pirate,hmpt_pirate1,ai_mpt_pirate1,hmpt_pirate2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d255af",
   "metadata": {},
   "source": [
    "#### Create the model and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d818fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "chain_pirate=LLMChain(llm=llm, prompt=chat_pirate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac784e84",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6addd31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be lovin' the art of code plunderin'.\n"
     ]
    }
   ],
   "source": [
    "response_pirate=chain_pirate.run(\"I love programming.\")\n",
    "print(response_pirate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379eb8c",
   "metadata": {},
   "source": [
    "### II. Can this be used for translation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30a838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpt_hindi=SystemMessagePromptTemplate.from_template('You are a helpful AI translator that translates English to Hindi and responds in Hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3bec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmpt_hindi1=HumanMessagePromptTemplate.from_template('Hello!')\n",
    "ai_mpt_hindi=AIMessagePromptTemplate.from_template('Namaste. Kya ho raha hai?')\n",
    "hmpt_hindi2=HumanMessagePromptTemplate.from_template('{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3336db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_hindi=ChatPromptTemplate(messages=[smpt_hindi,hmpt_hindi1,ai_mpt_hindi,hmpt_hindi2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4360ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_hindi=LLMChain(llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9),prompt=chat_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa86ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bahut accha hai ki aap Langchain sikh rahe hain aur pasand kar rahe hain! Kya aapko kisi tarah ki madad chahiye?\n"
     ]
    }
   ],
   "source": [
    "response_hindi=chain_hindi.run('I am learning Langchain and it is amazing!')\n",
    "print(response_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf1cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_mpt_hindi2=AIMessagePromptTemplate.from_template(response_hindi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69547d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='You are a helpful AI translator that translates English to Hindi and responds in Hindi', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Hello!', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Namaste. Kya ho raha hai?', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='{text}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Bahut accha hai ki aap Langchain sikh rahe hain aur pasand kar rahe hain! Kya aapko kisi tarah ki madad chahiye?', template_format='f-string', validate_template=True), additional_kwargs={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_hindi.messages.append(ai_mpt_hindi2)\n",
    "chat_hindi.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80d3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_hindi2=LLMChain(llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9),prompt=chat_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ca39a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain ke top 3 fayde ke baare mein baat karte hain:\n",
      "\n",
      "1. Bhasha Bhaagidari: Langchain aapko English aur Hindi ke beech bhasha bhaagidari ka anubhav pradan karta hai. Yeh aapko dono bhashaon mein swabhavik tarike se samjhne aur vyakt karne mein madad karta hai.\n",
      "\n",
      "2. Vyapak Pradarshan: Langchain aapko vyapak pradarshan pradan karta hai, jiske dwara aap bade ya chhote shabdon aur vakyon ka anuvad kar sakte hain. Yeh aapke vyaktitva ko badhata hai aur aapko aur vyapak tarike se bhasha ka upyog karne ki anumati deta hai.\n",
      "\n",
      "3. Samay aur Sram Bachao: Langchain ki sahayata se, aap samay aur sram dono bacha sakte hain. Aapko shabdon aur vakyon ko ek bhasha se doosre bhasha mein badalne ke liye alag-alag websites ya tools ko khojne ki zaroorat nahi hoti hai. Langchain ek hi jagah aapki sabhi anuvaad samasyaon ka samadhan pradan karta hai.\n",
      "\n",
      "Yeh thay Langchain ke top 3 fayde. Kripya bataiye agar aapko aur kisi vishay par jaankari chahiye.\n"
     ]
    }
   ],
   "source": [
    "response_hindi2=chain_hindi2.run('What do you think are the top 3 advantages of Langchain?')\n",
    "print(response_hindi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74c7b4",
   "metadata": {},
   "source": [
    "<b> Brilliant! </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4281c",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2980e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate,PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c4c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[\n",
    "    {'user':\"What's the weather like?\",\n",
    "    'response':\"It's raining cats and dogs, better bring an umbrella!\"},\n",
    "    \n",
    "    {'user':'How old are you?',\n",
    "    'response':\"Age is just a number, but I'm timeless.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d859ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt='''user:{user},\n",
    "    AI:{response}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3ba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff80477",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix='''User:{user}\\AI:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad715f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt_humor=FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=PromptTemplate(template=example_prompt,input_variables=['user','response']),\n",
    "        example_separator='\\n\\n',\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "        input_variables=['user']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "741a6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a666019",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_humor=LLMChain(prompt=fspt_humor,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa5e0780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The secret to happiness is to appreciate the little things in life.\n"
     ]
    }
   ],
   "source": [
    "response_humor=chain_humor.run(\"What's the secret to happiness?\")\n",
    "print(response_humor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aee5049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat=ChatOpenAI(model='gpt-3.5-turbo',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dc39950",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_humour2=LLMChain(prompt=fspt_humor,llm=llm_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd7fc3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret to happiness is a good sense of humor and a never-ending supply of chocolate.\n"
     ]
    }
   ],
   "source": [
    "response_humor2=chain_humour2.run(\"What's the secret to happiness?\")\n",
    "print(response_humor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a20e16",
   "metadata": {},
   "source": [
    "## Example Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595db5e",
   "metadata": {},
   "source": [
    "#### If we provide too many examples in the Few Shot Prompt selectors or if the length of each example is very long, there is a good chance that feeding all the examples to the model may exhaust the token limit of the model. Hence, we can select a subset of these examples to feed to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158817b",
   "metadata": {},
   "source": [
    "#### The subset of examples can be selected based on one of two criteria:\n",
    "<ol>\n",
    "    <li>The maximum length the example can be for it to be considered </li>\n",
    "    <li>The similarity of the example to the user query </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65763c68",
   "metadata": {},
   "source": [
    "### Semantic Similarity Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc347c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI, OpenAIChat\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2e91669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c609b",
   "metadata": {},
   "source": [
    "#### Setting up the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9cacee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://swamikannan/langchain_course_fewshot_selector\n",
      "Deep Lake Dataset in hub://swamikannan/langchain_course_fewshot_selector already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "USERID='swamikannan'\n",
    "PROJECT_NAME='langchain_course_fewshot_selector'\n",
    "url=f'hub://{USERID}/{PROJECT_NAME}'\n",
    "print(url)\n",
    "db=DeepLake(dataset_path=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447d3d4",
   "metadata": {},
   "source": [
    "#### Adding examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a09300",
   "metadata": {},
   "source": [
    "Examples obtained from <a href=\"https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb\"> Greg Kamradt's LangChain Cookbook Part 1 - Fundamentals </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb655ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_location = [\n",
    "    {\"item\": \"pirate\", \"location\": \"ship\"},\n",
    "    {\"item\": \"pilot\", \"location\": \"plane\"},\n",
    "    {\"item\": \"driver\", \"location\": \"car\"},\n",
    "    {\"item\": \"tree\", \"location\": \"ground\"},\n",
    "    {\"item\": \"bird\", \"location\": \"nest\"},\n",
    "    {\"item\": \"house\", \"location\":\"road\"},\n",
    "    {\"item\":\"worker\",\"location\":\"office\"},\n",
    "    {\"item\":\"sportsman\",\"location\":\"stadium\"},\n",
    "    {\"item\":\"singer\",\"location\":\"concert hall\"},\n",
    "    {\"item\":\"legislator\",\"location\":\"parliament\"},\n",
    "    {\"item\":\"thief\",\"location\":\"jail\"},\n",
    "    {\"item\":\"jewellery\",\"location\":\"safe\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cda4e",
   "metadata": {},
   "source": [
    "#### Adding prompt, prefix and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da55eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_location=PromptTemplate(\n",
    "    template='item:{item},location:{location}',\n",
    "    input_variables=['item','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5351e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='''You are an intelligent agent that gives us the location where an item is usually found. Some samples of these are as follows:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f0f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix='item:{item_no}\\nAI:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760c6fe",
   "metadata": {},
   "source": [
    "#### Create the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe54e8",
   "metadata": {},
   "source": [
    "The Semantic Similarity Example Selector will check similarity between the examples and the user query based on their token values i.e the mathematical representation of the text. Hence, we need to specify the appropriate algorithm to encode / embed the text to ensure a standard representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "864d003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74db4d7",
   "metadata": {},
   "source": [
    "#### Create SemanticSimilaritySelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45b121f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./deeplake/ already exists, loading from the storage\n",
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (179, 1536)  float32   None   \n",
      "    id        text      (179, 1)      str     None   \n",
      " metadata     json      (179, 1)      str     None   \n",
      "   text       text      (179, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "\r"
     ]
    }
   ],
   "source": [
    "sss_location=SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples_location,\n",
    "    embeddings=embeddings,\n",
    "    vectorstore_cls=db,\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2d7b2",
   "metadata": {},
   "source": [
    "#### Create the FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a3f2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt_location=FewShotPromptTemplate(\n",
    "    example_selector=sss_location,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    example_prompt=prompt_location,\n",
    "    input_variables=['item_no'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd7115",
   "metadata": {},
   "source": [
    "#### Create LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e8672ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_location=LLMChain(prompt=fspt_location,llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056777d0",
   "metadata": {},
   "source": [
    "#### Check which examples are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3c48224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent agent that gives us the location where an item is usually found. Some samples of these are as follows:\n",
      "\n",
      "item:worker,location:office\n",
      "\n",
      "item:worker,location:office\n",
      "\n",
      "item:blue collar worker\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(fspt_location.format(item_no='blue collar worker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739b0c3",
   "metadata": {},
   "source": [
    "#### It has understood the similarity between blue collar worker and worker but not that the last few examples (worker,sportsman, singer,legislator) are also mapped to their place of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93d3af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item_no': 'blue collar worker', 'text': 'location:factory'}\n"
     ]
    }
   ],
   "source": [
    "response_location1=chain_location('blue collar worker')\n",
    "print(response_location1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e62a8a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent agent that gives us the location where an item is usually found. Some samples of these are as follows:\n",
      "\n",
      "item:bird,location:nest\n",
      "\n",
      "item:bird,location:nest\n",
      "\n",
      "item:crow\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(fspt_location.format(item_no='crow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78dc58",
   "metadata": {},
   "source": [
    "#### This was a bit of a trick question since there were two examples: bird:nest and eagle:sky. One described where it rested and the other where it hunted. However, the similarity between 'crow' and 'bird' was identified but the fact that 'crow' and 'eagle' were both birds were missed out despite setting k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56d496b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_location1=chain_location.run('crow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b7eab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location:tree\n"
     ]
    }
   ],
   "source": [
    "print(response_location1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558955d",
   "metadata": {},
   "source": [
    "### Length based example Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a92f5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f534c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_emotion=[\n",
    "    {\"words\": \"happy\", \"antonyms\": \"sad\"},\n",
    "    {\"words\": \"tall\", \"antonyms\": \"short\"},\n",
    "    {\"words\": \"energetic\", \"antonyms\": \"lethargic\"},\n",
    "    {\"words\": \"sunny\", \"antonyms\": \"gloomy\"},\n",
    "    {\"words\": \"windy\", \"antonyms\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae94ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_emotion=PromptTemplate(\n",
    "    template='''word: {words}\n",
    "    antonym: {antonyms}''',\n",
    "    input_variables=['words','antonyms'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72a67f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_emotion='You are an AI agent who provides the antonym or the word which has an opposite meaning to the word provided. Samples of such antonyms are as follows:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3229f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_emotion='User:{words}\\nAI:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc012df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbes_emotion = LengthBasedExampleSelector(\n",
    "    examples=examples_emotion,\n",
    "    example_prompt=prompt_emotion,\n",
    "    max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21c94588",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt_emotion=FewShotPromptTemplate(\n",
    "    example_selector=lbes_emotion,\n",
    "    example_prompt=prompt_emotion,\n",
    "    prefix=prefix_emotion,\n",
    "    suffix=suffix_emotion,\n",
    "    input_variables=['words']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bdbb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_emotion=LLMChain(prompt=fspt_emotion,llm=OpenAI(model='text-davinci-003', temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55f43001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " small\n"
     ]
    }
   ],
   "source": [
    "response_emotion=chain_emotion.run('big')\n",
    "print(response_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4a2f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Unbake\n"
     ]
    }
   ],
   "source": [
    "response_emotion2=chain_emotion.run('bake')\n",
    "print(response_emotion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9a6f5",
   "metadata": {},
   "source": [
    "#### This is again interesting. Bake as a verb does not have an opposite. Baked as an adjective does.The opposite of baked goods is unbaked goods but \"unbake\" is not a verb. This gives us some clues about the limitations of the tokenization process for such models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
