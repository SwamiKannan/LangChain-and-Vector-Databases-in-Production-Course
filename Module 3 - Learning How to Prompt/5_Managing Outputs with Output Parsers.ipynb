{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfa6a55",
   "metadata": {},
   "source": [
    "# OutputParsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b423ec",
   "metadata": {},
   "source": [
    "#### Output parsers are a way of representing the output that the LLM returns to the user. <br />\n",
    "#### It is typically provided to the prompt as follows:\n",
    "1. Create an object of a class (user-defined or library)\n",
    "2. Pass the class as an argument to a parser object. Alternately, Langchain has also created parsers that don't need any arguments in case the parsing does not require too much complexity / flexibility\n",
    "3. Pass the parser object as an argument to the PromptTemplate as a partial_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1df7a2",
   "metadata": {},
   "source": [
    "This lesson takes on two major types of parsers:\n",
    "    <ol>\n",
    "    <li><b>Output Parsers</b></li>\n",
    "    <ul>\n",
    "        <li>Pydantic Parser</li>\n",
    "        <li>Comma Seperated List Output Parser </li>\n",
    "        <li>Stuctured Output Parser </li><br>\n",
    "    </ul>\n",
    "    <li><b>Error fixing parsers</b></li>\n",
    "    <ul>\n",
    "        <li>Output Fixing Parsers</li>\n",
    "        <li>Retry Output Parser </li>\n",
    "    </ul>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704812a",
   "metadata": {},
   "source": [
    "# A. Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eeade2",
   "metadata": {},
   "source": [
    "## 1. Pydantic parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0833e",
   "metadata": {},
   "source": [
    "Pydantic is the most widely used data validation library for Python. Data validation is a process that makes your data compliant with a set of rules, schemas or constraints that you defined for each attribute. This makes your code ingest and return data in the exact way it was expected to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e208d0",
   "metadata": {},
   "source": [
    "The PydanticOutputParser class instructs the model to generate its output in a JSON format and then extract the information from the response. You will be able to treat the parserâ€™s output as a list, meaning it will be possible to index through the results without worrying about formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf3d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116cbd3",
   "metadata": {},
   "source": [
    "### a. Creating the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6934544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b180d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(BaseModel):\n",
    "    #words is a list of strings with a (metadata) description\n",
    "    words: List[str] = Field(description=\"list of substitute words based on context\")\n",
    "        \n",
    "    @validator('words') #Validate the 'words' list as follows\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric(): #Check if the first letter in a string is a number\n",
    "                raise ValueError('The word cannot start with numbers')\n",
    "        return field      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b87100",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyparser=PydanticOutputParser(pydantic_object=Validator) #Create a PydanticOutputParser with the additional checker as defined by the Validator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb99380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"words\": {\"title\": \"Words\", \"description\": \"list of substitute words based on context\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"words\"]}\n",
      "```\n",
      "Object schema\n",
      "This is not a number\n"
     ]
    }
   ],
   "source": [
    "print(pyparser.get_format_instructions())\n",
    "print('Object schema')\n",
    "print(pyparser.pydantic_object.not_start_with_number('This is not a number'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9990dccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The word cannot start with numbers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Checking if the validator works. Expecting a ValueError\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpyparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_start_with_number\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m123Hello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mValidator.not_start_with_number\u001b[1;34m(cls, field)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m field:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misnumeric(): \u001b[38;5;66;03m#Check if the first letter in a string is a number\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe word cannot start with numbers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m field\n",
      "\u001b[1;31mValueError\u001b[0m: The word cannot start with numbers"
     ]
    }
   ],
   "source": [
    "#Checking if the validator works. Expecting a ValueError\n",
    "print(pyparser.pydantic_object.not_start_with_number('123Hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076a5d8",
   "metadata": {},
   "source": [
    "### b. Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2c6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='Output synonyms for the word {input_word} using the instructions {format_instructions} as per the {context}'\n",
    "syn_template=PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=['input_word','context'],\n",
    "            partial_variables={'format_instructions':pyparser.get_format_instructions()} #Basically get_format_instructions gets the instructions on how to format the LLM. Since the not_start_with_number is decorated with the @validator decorator, it will be incorporated \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bfde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = syn_template.format_prompt(\n",
    "    input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8577a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2356b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm(prompt=model_input.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e2aff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validator(words=['conduct', 'manner', 'action', 'demeanor', 'response', 'reaction', 'attitude'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyparser.parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bb046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input2 = syn_template.format_prompt(\n",
    "    input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae9aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=llm(prompt=model_input2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b7719a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validator(words=['conduct', 'action', 'demeanor', 'manner', 'attitude', 'response', 'reaction', 'activity', 'habit', 'practice'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyparser.parse(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55677a9c",
   "metadata": {},
   "source": [
    "'response', 'reaction', 'activity', 'habit' and 'practice' have all been added but what we were looking for was mostly around characteristics or laws or something similar. 'manner' seems to be appropriate though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26fb8e",
   "metadata": {},
   "source": [
    "### c. Requesting for and parsing multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d90f9",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the LLM for two outputs: the synonym AND the reasoning on why the synonym is appropriate\n",
    "In this case, we will run two checks on the output:\n",
    "<ol>\n",
    "    <li> Check if all synonyms have the appropriate structure i.e. not starting with a number (As before) </li>\n",
    "    <li> Checking if there is '.' after the reasoning indicating a complete sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c4012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation2(BaseModel):\n",
    "    word:List[str]=Field(description=\"list of substitute words based on context\")\n",
    "    response:List[str]=Field(description='reasoning why the substitute word is appropriate in the context')\n",
    "    \n",
    "    @validator('word')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric(): #Check if the first letter in a string is a number\n",
    "                raise ValueError('The word cannot start with numbers')\n",
    "        return field\n",
    "        \n",
    "    @validator('response')\n",
    "    def check_period(cls,field):\n",
    "        for item in field:\n",
    "            if item[-1] != '.':\n",
    "                item=item+'.'\n",
    "        return field\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "032f0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyparser2=PydanticOutputParser(pydantic_object=Validation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832c9b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"word\": {\"title\": \"Word\", \"description\": \"list of substitute words based on context\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"response\": {\"title\": \"Response\", \"description\": \"reasoning why the substitute word is appropriate in the context\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"word\", \"response\"]}\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyparser2.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dbc2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2='Output synonyms for the word {input_word} using the instructions {format_instructions} as per the {context} and provide appropriate explanation for why the synonym is appropriate'\n",
    "syn_template2=PromptTemplate(template=template2,\n",
    "                    input_variables=['input_word','context'],\n",
    "                    partial_variables={'format_instructions':pyparser2.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2d9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input3 = syn_template2.format_prompt(\n",
    "    input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b341f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3=llm(prompt=model_input3.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e8a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is the output instance:\n",
      "{\"word\": [\"conduct\", \"action\", \"demeanor\", \"manner\"], \"response\": [\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'conduct' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'action' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'demeanor' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'manner' is an appropriate synonym.\"]}\n"
     ]
    }
   ],
   "source": [
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86760f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed2=pyparser2.parse(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8340edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conduct', 'action', 'demeanor', 'manner']\n",
      "[\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'conduct' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'action' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'demeanor' is an appropriate synonym.\", \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'manner' is an appropriate synonym.\"]\n"
     ]
    }
   ],
   "source": [
    "print(parsed2.dict()['word'])\n",
    "print(parsed2.dict()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938bd0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.Word: conduct\tExplanation:The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'conduct' is an appropriate synonym.\n",
      "\n",
      "1.Word: action\tExplanation:The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'action' is an appropriate synonym.\n",
      "\n",
      "2.Word: demeanor\tExplanation:The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'demeanor' is an appropriate synonym.\n",
      "\n",
      "3.Word: manner\tExplanation:The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson, so 'manner' is an appropriate synonym.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,w in enumerate(zip(parsed2.dict()['word'],parsed2.dict()['response'])):\n",
    "    print(f'{i}.Word: {w[0]}\\tExplanation:{w[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54b8698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input4 = syn_template2.format_prompt(\n",
    "    input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6fe5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response4=llm(prompt=model_input4.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b89fc855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is the output instance:\n",
      "\n",
      "{\"word\": [\"action\", \"conduct\", \"demeanor\", \"manner\"], \"response\": [\"Action, conduct, demeanor, and manner are all synonyms for behaviour that can be used in the context of the planets' orbits around the sun, as they all refer to the way in which something is done or carried out.\"]}\n"
     ]
    }
   ],
   "source": [
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "715ee582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=['action', 'conduct', 'demeanor', 'manner'] response=[\"Action, conduct, demeanor, and manner are all synonyms for behaviour that can be used in the context of the planets' orbits around the sun, as they all refer to the way in which something is done or carried out.\"]\n"
     ]
    }
   ],
   "source": [
    "parsed4=pyparser2.parse(response4)\n",
    "print(parsed4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8e9851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.Word: action\tExplanation:Action, conduct, demeanor, and manner are all synonyms for behaviour that can be used in the context of the planets' orbits around the sun, as they all refer to the way in which something is done or carried out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,w in enumerate(zip(parsed4.dict()['word'],parsed4.dict()['response'])):\n",
    "    print(f'{i}.Word: {w[0]}\\tExplanation:{w[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813bff7",
   "metadata": {},
   "source": [
    "#### Interesting shortcut! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304809b2",
   "metadata": {},
   "source": [
    "## 2. Comma Separated List Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106a765",
   "metadata": {},
   "source": [
    "The CommaSeparatedListOutputParser parses the output of an LLM call to a comma-separated list. It does not need to be passed a class like a pydantic object due to its limited objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bda7fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15906c02",
   "metadata": {},
   "source": [
    "### a. Setting up the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abea247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cslop=CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11db5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1='Output synonyms for the word {input_word} using the instructions {format_instructions} as per the {context}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622dd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_syn_comma=PromptTemplate(template=template1,\n",
    "                               input_variables=['input_word', 'context'],\n",
    "                               partial_variables={'format_instructions':cslop.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a52855",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn_comma=prompt_syn_comma.format_prompt(input_word='behaviour',\n",
    "                                              context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc1cd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn_comma2 = prompt_syn_comma.format_prompt(\n",
    "    input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc00f9",
   "metadata": {},
   "source": [
    "### b. Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07771769",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f80249",
   "metadata": {},
   "source": [
    "### c. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9d60563",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_syn_comma=llm(model_syn_comma.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bb867f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unruly, rowdy, boisterous, ungovernable, unmanageable, wild, uncontrolled, rambunctious, turbulent, obstreperous.\n"
     ]
    }
   ],
   "source": [
    "print(response_syn_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdfe7920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unruly',\n",
       " 'rowdy',\n",
       " 'boisterous',\n",
       " 'ungovernable',\n",
       " 'unmanageable',\n",
       " 'wild',\n",
       " 'uncontrolled',\n",
       " 'rambunctious',\n",
       " 'turbulent',\n",
       " 'obstreperous.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cslop.parse(response_syn_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62729d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_syn_comma2=llm(model_syn_comma2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d409a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Conduct, deportment, demeanor, comportment, bearing, mien, attitude, posture, carriage, air, demeanor, demeanour\n"
     ]
    }
   ],
   "source": [
    "print(response_syn_comma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c36b2da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conduct',\n",
       " 'deportment',\n",
       " 'demeanor',\n",
       " 'comportment',\n",
       " 'bearing',\n",
       " 'mien',\n",
       " 'attitude',\n",
       " 'posture',\n",
       " 'carriage',\n",
       " 'air',\n",
       " 'demeanor',\n",
       " 'demeanour']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cslop.parse(response_syn_comma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c67da",
   "metadata": {},
   "source": [
    "## 3.Structured Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce61b7",
   "metadata": {},
   "source": [
    "This output parser can be used when you want to return <b>multiple fields</b>. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "998d5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b24b2c",
   "metadata": {},
   "source": [
    "### a. Setup parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad5883bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas=[\n",
    "    ResponseSchema(name='synonym',description='Output a synonym for the word provided'),\n",
    "    ResponseSchema(name='reason',description=\"share the reason why the model arrived at this synonym as an appropriate answer\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a800ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOP_syn=StructuredOutputParser.from_response_schemas(schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060f4ee",
   "metadata": {},
   "source": [
    "### b. Set up prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bf7f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='Output synonyms for the word {input_word} using the instructions {format_instructions} as per the {context} and provide appropriate explanation for why the synonym is appropriate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63266890",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_SOP1=PromptTemplate(template=template,\n",
    "                              input_variables=['input_word','context'],\n",
    "                              partial_variables={'format_instructions':SOP_syn.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "932fd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SOP1=prompt_SOP1.format_prompt(input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3bf5e",
   "metadata": {},
   "source": [
    "### c. Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c642d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ea9bc",
   "metadata": {},
   "source": [
    "### d. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "236016ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_SOP1=llm(model_SOP1.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c99aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"synonym\": \"conduct\",\n",
      "\t\"reason\": \"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson. This synonym is appropriate as it implies the same meaning as behaviour, which is the action or manner of conducting oneself.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response_SOP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aee5208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonym': 'conduct',\n",
       " 'reason': 'The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson. This synonym is appropriate as it implies the same meaning as behaviour, which is the action or manner of conducting oneself.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOP_syn.parse(response_SOP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ce5943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SOP2=prompt_SOP1.format_prompt(input_word=\"behaviour\",\n",
    "    context=\"The behaviour of the the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ec26eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Output synonyms for the word behaviour using the instructions The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"synonym\": string  // Output a synonym for the word provided\\n\\t\"reason\": string  // share the reason why the model arrived at this synonym as an appropriate answer\\n}\\n``` as per the The behaviour of the the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit and provide appropriate explanation for why the synonym is appropriate')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SOP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "729e4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_SOP2=llm(model_SOP2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f267242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"synonym\": \"conduct\",\n",
      "\t\"reason\": \"The behaviour of the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit, providing an appropriate explanation for why the synonym 'conduct' is appropriate.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response_SOP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82b6f843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonym': 'conduct',\n",
       " 'reason': \"The behaviour of the planets with respect to the sun has been well studied and tracked and has been shown to traverse an elliptical orbit, providing an appropriate explanation for why the synonym 'conduct' is appropriate.\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOP_syn.parse(response_SOP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb006c",
   "metadata": {},
   "source": [
    "# Error Fixing Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de4a1f",
   "metadata": {},
   "source": [
    "I got the examples in this section from Langchain's documentation since the examples were so much more illustrative:<br />\n",
    "<ul>\n",
    "    <li>Output fixing parser: https://python.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser</li>\n",
    "    <li>Retry parser: https://python.langchain.com/docs/modules/model_io/output_parsers/retry</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807b8a5",
   "metadata": {},
   "source": [
    "## OutputFixingParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43e8d1",
   "metadata": {},
   "source": [
    "We will create another Pydantic parser to attempt to parse an incorrectly formatted output from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a47b5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from pydantic import typing, BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7997e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Revalid(BaseModel):\n",
    "    name:List[str]= Field(description=\"name of an actor\")\n",
    "    film_names:List[str]=Field(description=\"list of names of films they starred in\")\n",
    "        \n",
    "pyrevalid=PydanticOutputParser(pydantic_object=Revalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cd9da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_query = \"Generate the filmography for a random actor.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e6da3",
   "metadata": {},
   "source": [
    "Let us now deliberately and manually create an incorrectly formatted LLM output (simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec3185c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "596e174c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Revalid from completion {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py:25\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     24\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m---> 25\u001b[0m json_object \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39mparse_obj(json_object)\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpyrevalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmisformatted\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse Revalid from completion {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "pyrevalid.parse(misformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242843dc",
   "metadata": {},
   "source": [
    "<b><font color=\"red\">The reason that this parsing throws an error is that JSON requires a double quote around strings.</font><br /></b>\n",
    "So instead of <b>'Tom Hanks'</b>, we need to write it as <b>\"Tom Hanks\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2c2e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "misformatted_correct = '{\"name\": [\"Tom Hanks\"], \"film_names\": [\"Forrest Gump\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b0c9345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Revalid(name=['Tom Hanks'], film_names=['Forrest Gump'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyrevalid.parse(misformatted_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc5f55",
   "metadata": {},
   "source": [
    "#### In order to automate similar formatting errors from the LLM output, we use a OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30319d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bb4de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofp=OutputFixingParser.from_llm(parser=pyrevalid,llm=llm) #Take the retry_chain etc values from the LLM  object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3050e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Revalid(name=['Tom Hanks'], film_names=['Forrest Gump'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofp.parse(misformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf56c3",
   "metadata": {},
   "source": [
    "TADA ! The OutputFixingParser automatically corrects the formatting of the output to ensure compatibility to the JSON format before passing it to the PydanticOutputParser for the final parsing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6caf4a",
   "metadata": {},
   "source": [
    "### RetryWithErrorOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f4c8f",
   "metadata": {},
   "source": [
    "In some cases, the parser needs access to both the output and the prompt to process the full context.An example of this is when __the output is not just in the incorrect format, but is partially complete.__ Taking one example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e175cfd",
   "metadata": {},
   "source": [
    "### Scenario 1: Parsing with simple PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87273102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from pydantic import BaseModel, validator, typing, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4df4c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(BaseModel):\n",
    "    action:str=Field(description=\"action to take\")\n",
    "    trigger:str=Field(description=\"input for action\")\n",
    "\n",
    "pyaction=PydanticOutputParser(pydantic_object=Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "262cd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_action=PromptTemplate(template='Answer the user query.\\n{format_instructions}\\n{query}\\n',\n",
    "                     input_variables=['query'],\n",
    "                     partial_variables={'format_instructions':pyaction.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bdca237",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63ca3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"action\": {\"title\": \"Action\", \"description\": \"action to take\", \"type\": \"string\"}, \"trigger\": {\"title\": \"Trigger\", \"description\": \"input for action\", \"type\": \"string\"}}, \"required\": [\"action\", \"trigger\"]}\\n```\\nwho is leo di caprios gf?\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_action.format_prompt(query=\"who is leo di caprios gf?\").to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b5b8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_action=llm(prompt_action.format_prompt(query=\"who is leo di caprios gf?\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1c49959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"action\": \"find\", \"trigger\": \"Leo DiCaprio\\'s girlfriend\"}'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f31f0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action(action='find', trigger=\"Leo DiCaprio's girlfriend\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyaction.parse(response_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f41d9",
   "metadata": {},
   "source": [
    "__This is the default answer__. <br />\n",
    "***\n",
    "Now let us take an example of an incomplete response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f9f4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_answer='\\n{\"action\": \"find\"}' #Only the action response is received"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60aea53",
   "metadata": {},
   "source": [
    "If we try to parse this, we should get a validation error stating that the required field 'trigger' is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa78d76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Action from completion \n{\"action\": \"find\"}. Got: 1 validation error for Action\ntrigger\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     25\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\pydantic\\main.py:526\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Action\ntrigger\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpyaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_answer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse Action from completion \n{\"action\": \"find\"}. Got: 1 validation error for Action\ntrigger\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "pyaction.parse(bad_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ad679",
   "metadata": {},
   "source": [
    "Hence, we know that PydanticOutputParser cannot solve this issue. Now what if we try a simple __OutputFixingParser__ like before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbbdbd",
   "metadata": {},
   "source": [
    "### Scenario 2: Parsing the output using OutputFixingParser to help the PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57ad2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e48303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofp_action=OutputFixingParser.from_llm(parser=pyaction,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f58c0063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action(action='find', trigger='file')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofp_action.parse(bad_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc4c1f",
   "metadata": {},
   "source": [
    "OutputFixingParser has simply filled a default value of 'file' as it is unable to identify what the trigger should be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6fc18",
   "metadata": {},
   "source": [
    "Now, let us try RetryWithOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cf09e",
   "metadata": {},
   "source": [
    "### Scenario 3: Using the RetryWithErrorOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d729e",
   "metadata": {},
   "source": [
    "__When we create the RetryWithErrorOutputParser, we also pass the string value of the prompt to the parser so that it can reference the original prompt to assess what is missing and fill it appropriately.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ccfa843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import RetryWithErrorOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d577a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rweop_action=RetryWithErrorOutputParser.from_llm(parser=pyaction,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "19786a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value=prompt_action.format_prompt(query=\"who is leo di caprios gf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2bf9cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_rweop=rweop_action.parse_with_prompt(bad_answer, prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9ac8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action(action='find', trigger=\"Leo DiCaprio's girlfriend\")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_rweop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4c485",
   "metadata": {},
   "source": [
    "__This is the same as the original answer we obtained by passing in the complete data to the LLM__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
