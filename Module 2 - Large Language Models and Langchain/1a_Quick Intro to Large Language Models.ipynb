{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93d31f0",
   "metadata": {},
   "source": [
    "## Tracking token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582b3f7",
   "metadata": {},
   "source": [
    "Langchain calculates the token usage i.e. charges for using the OpenAI API without querying OpenAI itself! The prices of all the models are configured at:\n",
    "    https://api.python.langchain.com/en/latest/_modules/langchain/callbacks/openai_info.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fda4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".code_html p {\n",
       "    font-family: \"Inconsolata\", ui-monospace;\n",
       ".rendered_html p {\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".code_html p {\n",
    "    font-family: \"Inconsolata\", ui-monospace;\n",
    ".rendered_html p {\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cfd21",
   "metadata": {},
   "source": [
    "As of today (June 30, 2023, the prices configured are as follows:<br>\n",
    "<b>MODEL_COST_PER_1K_TOKENS</b> = {<br>\n",
    "    # GPT-4 input<br>\n",
    "    \"gpt-4\": 0.03,<br>\n",
    "    \"gpt-4-0314\": 0.03,<br>\n",
    "    \"gpt-4-0613\": 0.03,<br>\n",
    "    \"gpt-4-32k\": 0.06,<br>\n",
    "    \"gpt-4-32k-0314\": 0.06,<br>\n",
    "    \"gpt-4-32k-0613\": 0.06,<br>\n",
    "   <b> # GPT-4 output</b><br>\n",
    "    \"gpt-4-completion\": 0.06,<br>\n",
    "    \"gpt-4-0314-completion\": 0.06,<br>\n",
    "    \"gpt-4-0613-completion\": 0.06,<br>\n",
    "    \"gpt-4-32k-completion\": 0.12,<br>\n",
    "    \"gpt-4-32k-0314-completion\": 0.12,<br>\n",
    "    \"gpt-4-32k-0613-completion\": 0.12,<br>\n",
    "   <b> # GPT-3.5 input</b><br>\n",
    "    \"gpt-3.5-turbo\": 0.0015,<br>\n",
    "    \"gpt-3.5-turbo-0301\": 0.0015,<br>\n",
    "    \"gpt-3.5-turbo-0613\": 0.0015,<br>\n",
    "    \"gpt-3.5-turbo-16k\": 0.003,</b><br>\n",
    "    \"gpt-3.5-turbo-16k-0613\": 0.003,<br>\n",
    " <b>   # GPT-3.5 output</b><br>\n",
    "    \"gpt-3.5-turbo-completion\": 0.002,<br>\n",
    "    \"gpt-3.5-turbo-0301-completion\": 0.002,<br>\n",
    "    \"gpt-3.5-turbo-0613-completion\": 0.002,<br>\n",
    "    \"gpt-3.5-turbo-16k-completion\": 0.004,<br>\n",
    "    \"gpt-3.5-turbo-16k-0613-completion\": 0.004,<br>\n",
    "   <b> # Others</b><br>\n",
    "    \"gpt-35-turbo\": 0.002,  # Azure OpenAI version of ChatGPT<br>\n",
    "    \"text-ada-001\": 0.0004,<br>\n",
    "    \"ada\": 0.0004,<br>\n",
    "    \"text-babbage-001\": 0.0005,<br>\n",
    "    \"babbage\": 0.0005,<br>\n",
    "    \"text-curie-001\": 0.002,<br>\n",
    "    \"curie\": 0.002,<br>\n",
    "    \"text-davinci-003\": 0.02,<br>\n",
    "    \"text-davinci-002\": 0.02,<br>\n",
    "    \"code-davinci-002\": 0.02,<br>\n",
    "    \"ada-finetuned\": 0.0016,<br>\n",
    "    \"babbage-finetuned\": 0.0024,<br>\n",
    "    \"curie-finetuned\": 0.012,<br>\n",
    "    \"davinci-finetuned\": 0.12,<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f8a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.7) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936a032",
   "metadata": {},
   "source": [
    "### Get OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275274ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model=\"text-davinci-003\", temperature=0.9, n=4, best_of=4)\n",
    "#best_of: generates best_of completions server-side and returns the \"best\"\n",
    "#n: int = 1: How many completions to generate for each prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece4af1",
   "metadata": {},
   "source": [
    "<b>best_of</b>: generates best_of completions server-side and returns the \"best\"<br>\n",
    "<b>n: int = 1:</b> How many completions to generate for each prompt.<br>\n",
    "\n",
    "If <b>n</b> and <b>best_of</b> both equal 1 (which is the default), the number of generated tokens will be at most, equal to max_tokens.<br>\n",
    "If <b>n</b> (the number of completions returned) or <b>best_of</b> (the number of completions generated for consideration) are set to > 1, each request will create multiple outputs. <br>\n",
    "Here, you can consider the number of generated tokens as [ max_tokens * max (n, best_of) ] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495ce923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result=llm('What are three creative name for a scuba diving center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95335ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Abyss Adventures\n",
      "2. Subaquatic Explorers\n",
      "3. Aqua Wonders\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54b3ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 99\n",
      "\tPrompt Tokens: 11\n",
      "\tCompletion Tokens: 88\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00198\n"
     ]
    }
   ],
   "source": [
    "print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476b0d3",
   "metadata": {},
   "source": [
    "## Few Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92011df1",
   "metadata": {},
   "source": [
    "Few shot learning is a type of query where apart from the prompt and the input variables, we also provide a few examples that the model can use to understand the \"context\" or the pattern within the expected responses vis-a-vis the user input.<br>\n",
    "A FewShotPromptTemplate is basically used to create a prompt of the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e278823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6ab3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inital prompt template that will be used for user inputs\n",
    "template='''User: {query}\\n\\n\n",
    "                {response}'''\n",
    "input_var=['query','response'] #For the samples we will be providing 'response' so that will act as a template as well\n",
    "prompt_template=PromptTemplate(input_variables=input_var,template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b4c6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[{'query':'Bob has 36 candy bars. He eats 29 of them. What does he have now?','response':'Bob has diabetes'},\n",
    "        {'query':'There are 4 ghosts. One flies away. How many are left?','response':'None. Because ghosts are not real'}]\n",
    "prefix='You are an LLM model that gives answers that are correct logically but are extremely funny. You are known for your wit, humor and creativity. Examples of your response are below:'\n",
    "suffix='Respond to the question in the format user:{query} AI:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f77d7",
   "metadata": {},
   "source": [
    "#### Testing our template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "60d16f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Bob has 36 candy bars. He eats 29 of them. What does he have now?\n",
      "\n",
      "\n",
      "                Bob has diabetes\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(**samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53ac2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "fspt=FewShotPromptTemplate(input_variables=['query'],examples=samples,prefix=prefix, suffix=suffix, example_prompt=prompt_template,example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96739f7a",
   "metadata": {},
   "source": [
    "<b>input_variables</b>: The variable as defined in your suffix and hence, your overall FewShotPromptTemplate prompt<br>\n",
    "<b>prefix</b>: This can be used to setup your AI assistant. The text here will help the model understand what it is the meta data. In this example, we give the model a \"personality\"<br>\n",
    "<b>suffix</b>: This acts as an pseudo-output parser and helps format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "83ca7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=[{'query': 'Bob has 36 candy bars. He eats 29 of them. What does he have now?', 'response': 'Bob has diabetes'}, {'query': 'There are 4 ghosts. One flies away. How many are left?', 'response': 'None. Because ghosts are not real'}] example_selector=None example_prompt=PromptTemplate(input_variables=['query', 'response'], output_parser=None, partial_variables={}, template='User: {query}\\n\\n\\n                {response}', template_format='f-string', validate_template=True) suffix='Respond to the question in the format user:{query} AI:' example_separator='\\n\\n' prefix='You are an LLM model that gives answers that are correct logically but are extremely funny. You are known for your wit, humor and creativity. Examples of your response are below:' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(fspt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea3eec",
   "metadata": {},
   "source": [
    "#### Check how the FewShotPromptTemplate works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9ecf61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query='What was the first thing that Queen Elizabeth did when she ascended the throne?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7d0f4",
   "metadata": {},
   "source": [
    "#### What does the final prompt to the model look like?m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5a985f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an LLM model that gives answers that are correct logically but are extremely funny. You are known for your wit, humor and creativity. Examples of your response are below:\\n\\nUser: Bob has 36 candy bars. He eats 29 of them. What does he have now?\\n\\n\\n                Bob has diabetes\\n\\nUser: There are 4 ghosts. One flies away. How many are left?\\n\\n\\n                None. Because ghosts are not real\\n\\nRespond to the question in the format user:['What was the first thing that Queen Elizabeth did when she ascended the throne?'] AI:\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fspt.format(query=[user_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c50d9",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fcd14526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87527b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a077dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "User: What was the first thing that Queen Elizabeth did when she ascended the throne?\n",
      "\n",
      "AI: She declared, \"I shall be known as the Queen of puns!\"\n"
     ]
    }
   ],
   "source": [
    "print(llm(fspt.format(query=user_query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb064f",
   "metadata": {},
   "source": [
    "#### We can also run the model using a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be2d2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "596f6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chain=LLMChain(llm=llm, prompt=fspt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ebb7bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query='How can Columbia control the problem of guerrillas taking over the country?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7173abb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUser: How can Columbia control the problem of guerrillas taking over the country?\\n\\nAI: By offering them a better job opportunity, like becoming a professional ghostbuster!'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_chain.run(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a498e",
   "metadata": {},
   "source": [
    "The \\n\\n is basically a formatting issue. It disappears if we use:<br><br> <i>print(prompt_chain.run(user_query)</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
