{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0138f5",
   "metadata": {},
   "source": [
    "# LLMs vs. Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866dc406",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ae4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.7) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bbc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='Provide 4 suitable names for a store that sells {product} in a bulleted list'\n",
    "input_variables=['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3157dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    input_variables=input_variables,\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f257441",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b456207",
   "metadata": {},
   "source": [
    "#### Directly run the query without using a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29396a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm(prompt.format_prompt(product='graphics cards').to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3ef4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "• Graphics Galore\n",
      "• GPU Heaven\n",
      "• Video Card Vault\n",
      "• Graphics Card Emporium\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d42c9c",
   "metadata": {},
   "source": [
    "#### Run the query using a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17973ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d7ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2026bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n• Graphics Galore\\n• GPU Heaven\\n• Video Card Vault\\n• Graphics Card Emporium'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run('graphics cards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dc5e1",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47867dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909334b",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfa0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63117c62",
   "metadata": {},
   "source": [
    "#### Create the chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1aa8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message='You are an AI assistant that translates English to French'\n",
    "human_message='Translate the following sentence: to code is human, to LLM is divine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9517486",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_template=SystemMessage(content=sys_message)\n",
    "human_template=HumanMessage(content=human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ab2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[sys_template,human_template]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd5c82",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9b3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5956a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coder est humain, faire un LLM est divin.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c7fc",
   "metadata": {},
   "source": [
    "#### Run the prompt for multiple messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4906abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message2=['You cannot step in the same river twice.',\n",
    "                'Death need not concern us because when we exist death does not, and when death exists we do not.',\n",
    "               'God is dead.',\n",
    "               'The unexamined life is not worth living.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e7dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template2=HumanMessage(content='Translate the following sentences:'+'\\n'.join(human_message2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8389ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_templates=[[sys_template,HumanMessage(content=c)] for c in human_message2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "032c6442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SystemMessage(content='You are an AI assistant that translates English to French', additional_kwargs={}),\n",
       "  HumanMessage(content='You cannot step in the same river twice.', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='You are an AI assistant that translates English to French', additional_kwargs={}),\n",
       "  HumanMessage(content='Death need not concern us because when we exist death does not, and when death exists we do not.', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='You are an AI assistant that translates English to French', additional_kwargs={}),\n",
       "  HumanMessage(content='God is dead.', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='You are an AI assistant that translates English to French', additional_kwargs={}),\n",
       "  HumanMessage(content='The unexamined life is not worth living.', additional_kwargs={}, example=False)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d16d4",
   "metadata": {},
   "source": [
    "<b>Previously, we passed a list of questions to llm.generate() so that it could answer them one after another. <br><br>Similarly, here we pass another list wherein each item of the list is [system_template, human_template_sample] (where human_template_sample is one item in the human_template2 list)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6eba5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.generate(human_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "062708df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='On ne peut pas marcher deux fois dans la même rivière.', generation_info=None, message=AIMessage(content='On ne peut pas marcher deux fois dans la même rivière.', additional_kwargs={}, example=False))], [ChatGeneration(text=\"La mort ne doit pas nous préoccuper car lorsque nous existons, la mort n'existe pas, et lorsque la mort existe, nous n'existons pas.\", generation_info=None, message=AIMessage(content=\"La mort ne doit pas nous préoccuper car lorsque nous existons, la mort n'existe pas, et lorsque la mort existe, nous n'existons pas.\", additional_kwargs={}, example=False))], [ChatGeneration(text='Dieu est mort.', generation_info=None, message=AIMessage(content='Dieu est mort.', additional_kwargs={}, example=False))], [ChatGeneration(text=\"La vie non examinée ne vaut pas la peine d'être vécue.\", generation_info=None, message=AIMessage(content=\"La vie non examinée ne vaut pas la peine d'être vécue.\", additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 128, 'completion_tokens': 73, 'total_tokens': 201}, 'model_name': 'gpt-3.5-turbo'} run=RunInfo(run_id=UUID('75d2b42f-6993-4c87-afd5-16b16608d0ea'))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b410df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On ne peut pas marcher deux fois dans la même rivière.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()['generations'][0][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d300419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: You cannot step in the same river twice.\n",
      "French: On ne peut pas marcher deux fois dans la même rivière.\n",
      "\n",
      "English: Death need not concern us because when we exist death does not, and when death exists we do not.\n",
      "French: La mort ne doit pas nous préoccuper car lorsque nous existons, la mort n'existe pas, et lorsque la mort existe, nous n'existons pas.\n",
      "\n",
      "English: God is dead.\n",
      "French: Dieu est mort.\n",
      "\n",
      "English: The unexamined life is not worth living.\n",
      "French: La vie non examinée ne vaut pas la peine d'être vécue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e,f in zip(human_message2,response.dict()['generations']):\n",
    "    print(f'English: {e}')\n",
    "    print(f\"French: {f[0]['text']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8aa061b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 128,\n",
       "  'completion_tokens': 73,\n",
       "  'total_tokens': 201},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()['llm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9e1c53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': UUID('75d2b42f-6993-4c87-afd5-16b16608d0ea')}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()['run']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb367556",
   "metadata": {},
   "source": [
    "#### Trying the same using a standard LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ab9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "885a39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='You are an AI assistant that translates the {sentences} from English to French'\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['sentences'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f647232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "567aeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(prompt=prompt,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d86b3bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=PromptTemplate(input_variables=['sentences'], output_parser=None, partial_variables={}, template='You are an AI assistant that translates the {sentences} from English to French', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bbe40dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You cannot step in the same river twice.',\n",
       " 'Death need not concern us because when we exist death does not, and when death exists we do not.',\n",
       " 'God is dead.',\n",
       " 'The unexamined life is not worth living.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c180f55",
   "metadata": {},
   "source": [
    "llmchain.generate() requires a list of dictionaries to process. So we need to convert human_message2 to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d03ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentences': 'You cannot step in the same river twice.'},\n",
       " {'sentences': 'Death need not concern us because when we exist death does not, and when death exists we do not.'},\n",
       " {'sentences': 'God is dead.'},\n",
       " {'sentences': 'The unexamined life is not worth living.'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_ms_dict=[{'sentences':i}for i in human_message2]\n",
    "human_ms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86e2c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llmchain.generate(input_list=human_ms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d7e3217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vous ne pouvez pas marcher dans la même rivière deux fois.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations[0][0].dict()['text'][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b28f4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: The unexamined life is not worth living.\n",
      "French as per chat: On ne peut pas marcher deux fois dans la même rivière.\n",
      "French as per LLM: Vous ne pouvez pas marcher dans la même rivière deux fois.\n",
      "\n",
      "English: The unexamined life is not worth living.\n",
      "French as per chat: La mort ne doit pas nous préoccuper car lorsque nous existons, la mort n'existe pas, et lorsque la mort existe, nous n'existons pas.\n",
      "French as per LLM: La mort ne nous concerne pas car lorsque nous existons la mort n'existe pas, et lorsque la mort existe nous n'existons pas.\n",
      "\n",
      "English: The unexamined life is not worth living.\n",
      "French as per chat: Dieu est mort.\n",
      "French as per LLM: Dieu est mort.\n",
      "\n",
      "English: The unexamined life is not worth living.\n",
      "French as per chat: La vie non examinée ne vaut pas la peine d'être vécue.\n",
      "French as per LLM: La vie non examinée n'est pas digne d'être vécue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en,fr_chat,fr_llm in zip(human_message2,response.dict()['generations'],result.generations):\n",
    "    print(f'English: {e}')\n",
    "    print(f\"French as per chat: {fr_chat[0]['text']}\")\n",
    "    print(f'French as per LLM: {fr_llm[0].dict()[\"text\"][2:]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
