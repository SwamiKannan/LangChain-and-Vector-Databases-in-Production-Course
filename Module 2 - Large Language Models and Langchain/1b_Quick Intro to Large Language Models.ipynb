{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f3e630",
   "metadata": {},
   "source": [
    "## Question Answering Template using HuggingFaceHub (Google FLAN-T5 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef049f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\langchain\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.7) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "bdbc4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5d0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe9a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    template='''Question: {question}\n",
    "    Answer:'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35db67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(llm=llm,prompt=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daa76a",
   "metadata": {},
   "source": [
    "#### Answering one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06840402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run(question='What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f83ae2",
   "metadata": {},
   "source": [
    "#### Answering multiple questions in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08157c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[\n",
    "    {'question':'What is the capital of France?'},\n",
    "    {'question':'Where can we observe whale sharks in South America?'},\n",
    "    {'question':\"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question':'What is the most efficient fuel for rockets to the International Space Station?'}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fef1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llmchain.generate(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "089e2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='pacific', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='kerosene', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('e3ef3c59-2d28-4bcc-9542-d106b7f8306d'))\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c59c5662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'paris', 'generation_info': None}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dict()['generations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f758ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: paris\n",
      "**************\n",
      "Question: Where can we observe whale sharks in South America?\n",
      "Answer: pacific\n",
      "**************\n",
      "Question: Which gas is most abundant in Earth's atmosphere?\n",
      "Answer: nitrogen\n",
      "**************\n",
      "Question: What is the most efficient fuel for rockets to the International Space Station?\n",
      "Answer: kerosene\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "for q,a in zip(questions, result.dict()['generations']):\n",
    "    print(f'Question: {q[\"question\"]}')\n",
    "    print(f'Answer: {a[0][\"text\"]}')\n",
    "    print('**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61592ec",
   "metadata": {},
   "source": [
    "#### Trying to pass all the questions in llmchain.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea720a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=llmchain.run(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748710f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helium'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338c3b6",
   "metadata": {},
   "source": [
    "#### Interestingly, we get only one answer if we send all 4 questions in a chain.run command <br>\n",
    "The main reason is that all 4 q's are pushed into a single context or as a single question which does not make sense. And for some reason, the answer 'helium is irrelevant to all :)<br>\n",
    "To sort this out, we need to update the template and hence, the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ce72c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "input_variables=['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "37046c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(input_variables=input_variables, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "0e296441",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dd4b4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[\n",
    "    {'question':\"What is the capital of France?\"},\n",
    "    {'question':\"Where can we observe sharks?\"},\n",
    "    {'question':'What is the most efficient fuel for rockets to the International Space Station?'}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f04e01ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the capital of France?'},\n",
       " {'question': 'Where can we observe sharks?'},\n",
       " {'question': 'What is the most efficient fuel for rockets to the International Space Station?'}]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "adc3fec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the capital of France?',\n",
       " 'Where can we observe sharks?',\n",
       " 'What is the most efficient fuel for rockets to the International Space Station?']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_list=[q['question'] for q in questions]\n",
    "q_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2951d497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?\\nWhere can we observe sharks?\\nWhat is the most efficient fuel for rockets to the International Space Station?'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_str='\\n'.join([q['question'] for q in questions])\n",
    "q_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d59dd2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the capital of France. Sharks live in the ocean. Sharks live in the'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run(questions=q_str,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33df4",
   "metadata": {},
   "source": [
    "Hence, we see that the prompting for multiple questions is not as robust as chain.generate().<br>\n",
    "The llmchain literally starts repeating out the second answer instead of answering the third question <br>\n",
    "The third question is completely ignored. Could this be because of a token limit? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0babf",
   "metadata": {},
   "source": [
    "#### Now, modifying the questions slightly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c85d0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions2=[\n",
    "    {'question':\"What is the capital of France?\"},\n",
    "    {'question':\"Where can we observe sharks?\"},\n",
    "    {'question':'What is the most efficient fuel for rockets?'}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b3d21b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?Where can we observe sharks?What is the most efficient fuel for rockets?'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_str2=''.join([q['question'] for q in questions2])\n",
    "q_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c2d6481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. The most efficient fuel for rockets is hydrogen. Hydrogen'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.predict(questions=q_str2,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0955ae4",
   "metadata": {},
   "source": [
    "Hence, we see that the prompting for multiple questions is not as robust as chain.generate().<br>\n",
    "The llmchain literally misses out the second question<br>\n",
    "The third question has a different answer just because we ignored the \"International Space Station\" part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88247ce",
   "metadata": {},
   "source": [
    "<b>Hence, it makes sense to segregate the questions and pass them individually to the model.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663a99d",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701592ca",
   "metadata": {},
   "source": [
    "#### Text summarization can be done by simply prompting the model to summarize the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "9f90d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "14b73f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0e2f5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "input_variables=['text'],\n",
    "template='Summarize the following text to one sentence: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "92fca636",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text='LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "502cedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original text: 448\n"
     ]
    }
   ],
   "source": [
    "print('Length of original text:',len(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "93186beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize the following text to one sentence: LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "21c6c9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain offers various modules for building language model applications, allowing users to combine them for more complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of creating a company name based on its product.'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(prompt.format(text=sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3cce7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "373cd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(prompt=prompt,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c978f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ=llmchain.run(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "acadfe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of summarized text: 319\n"
     ]
    }
   ],
   "source": [
    "print('Length of summarized text:',len(summ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26969d99",
   "metadata": {},
   "source": [
    "#### Hence, we can see the summarization has reduced the length of the text from 448 to 319 - a difference of around 29%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb8801",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8a77e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9a4a6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5313ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "template='Translate the sentence {sentence} from {source_language} to {target_language}',\n",
    "input_variables=['sentence','source_language','target_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "49b218da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain=LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "df354500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The natural language processing is the new PC interface.'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.run(sentence='Le traitement du langage naturel est la nouvelle interface PC',source_language=\"French\",target_language='English')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
